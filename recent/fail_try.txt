[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[47] A. Komatsuzaki et al., "Sparse upcycling: Training mixture-of-experts from dense checkpoints." in Proc. 11th Int. Conf. Learn. Representations, 2022.
[48] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "MoEfication: Transformer feed-forward layers are mixtures of experts," in Proc. Findings Assoc. Comput. Linguistics, 2022, pp. 877-890.
[27] xAI, "Grok-1." Mar. 2024. [Online]. Available: [https://github.com/xai-org/grok-1](https://github.com/xai-org/grok-1)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[29] S. A. R. Team, "Snowflake Arctic: The best LLM for enterprise AIâ€”efficiently intelligent, truly open" Apr. 2024. [Online]. Available: [https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/)
[47] A. Komatsuzaki et al., "Sparse upcycling: Training mixture-of-experts from dense checkpoints." in Proc. 11th Int. Conf. Learn. Representations, 2022.
[48] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "MoEfication: Transformer feed-forward layers are mixtures of experts," in Proc. Findings Assoc. Comput. Linguistics, 2022, pp. 877-890.
[49] L.-M. Team, "LLAMA-MOE: Building mixture-of-experts from LLAMA with continual pre-training." Dec. 2023. [Online]. Available: [https://github.com/pjlab-sys4nlp/llama-moe](https://github.com/pjlab-sys4nlp/llama-moe)
[50] F. Xue, X. He, X. Ren, Y. Lou, and Y. You, "One student knows all experts know: From sparse to dense," 2022, arXiv:2201,10890.
[51] T. Chen et al., "Task-specific expert pruning for sparse mixture-of-experts," 2022, arXiv:2206.00277.
[52] S. Sukhbaatar et al., "Branch-train-MiX: Mixing expert LLMs into a mixture-of-experts LLM," 2024, arXiv: 2403.07816.
[53] W. Chen et al., "Lifelong language pretraining with distribution-specialized experts," in Proc. 40th Int. Conf. Mach. Learn., 2023, pp. 5383-5395.
[54] S. Antoniak et al., "Mixture of tokens: Efficient LLMs through cross-example aggregation," 2023, arXiv: 2310.15961.
[55] D. Raposo et al., "Mixture-of-depths: Dynamically allocating compute in transformer-based language models," 2024, arXiv: 2404.02258.
[56] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," in Proc. AAAI Conf. Artif. Intell., 2022, pp. 8779-8787.
[57] S. Tan, Y. Shen. Z. Chen, A. Courville, and C. Gan, "Sparse universal transformer," in Proc. Conf. Empirical Methods Natural Lang. Process., 2023, pp. 169-179.
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[28] Databricks, "Introducing DBRX: A new State-of-the-Art open LLM," Mar. 2024. [Online]. Available: [https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm](https://www.google.com/search?q=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-Ilm)
[93] D. Li et al., "Semantic distance organizes social knowledge: Insights from semantic dementia and cross-modal conceptual space" 2024, arXiv: 2404.15151.
[106] L.. Shen et al., "Moesys: A distributed and efficient mixture-of-experts training and inference system for internet services" 2022, arXiv:2205.10034.