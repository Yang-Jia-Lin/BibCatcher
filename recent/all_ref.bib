@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 journal = {Advances in neural information processing systems},
 title = {Attention is all you need},
 volume = {30},
 year = {2017}
}

@article{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 journal = {Advances in neural information processing systems},
 pages = {1877--1901},
 title = {Language models are few-shot learners},
 volume = {33},
 year = {2020}
}

@article{chowdhery2023palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {Journal of Machine Learning Research},
 number = {240},
 pages = {1--113},
 shorttitle = {Palm},
 title = {Palm: Scaling language modeling with pathways},
 volume = {24},
 year = {2023}
}

@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {arXiv preprint arXiv:2303.08774},
 title = {Gpt-4 technical report},
 year = {2023}
}

@article{jiang2024survey,
 author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
 journal = {arXiv preprint arXiv:2406.00515},
 title = {A survey on large language models for code generation},
 year = {2024}
}

@article{riquelme2021scaling,
 author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
 journal = {Advances in Neural Information Processing Systems},
 pages = {8583--8595},
 title = {Scaling vision with sparse mixture of experts},
 volume = {34},
 year = {2021}
}

@inproceedings{liu2021swin,
 author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
 booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
 pages = {10012--10022},
 title = {Swin transformer: Hierarchical vision transformer using shifted windows},
 year = {2021}
}

@article{lu2019vilbert,
 author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
 journal = {Advances in neural information processing systems},
 shorttitle = {Vilbert},
 title = {Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
 volume = {32},
 year = {2019}
}

@article{zhou2022learning,
 author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
 journal = {International Journal of Computer Vision},
 number = {9},
 pages = {2337--2348},
 publisher = {Springer},
 title = {Learning to prompt for vision-language models},
 volume = {130},
 year = {2022}
}

@article{zhu2023minigpt,
 author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2304.10592},
 shorttitle = {Minigpt-4},
 title = {Minigpt-4: Enhancing vision-language understanding with advanced large language models},
 year = {2023}
}

@article{kaplan2020scaling,
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 journal = {arXiv preprint arXiv:2001.08361},
 title = {Scaling laws for neural language models},
 year = {2020}
}

@article{wei2022emergent,
 author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
 journal = {arXiv preprint arXiv:2206.07682},
 title = {Emergent abilities of large language models},
 year = {2022}
}

@article{yoo2024hyperclova,
 author = {Yoo, Kang Min and Han, Jaegeun and In, Sookyo and Jeon, Heewon and Jeong, Jisu and Kang, Jaewook and Kim, Hyunwook and Kim, Kyung-Min and Kim, Munhyong and Kim, Sungju and others},
 journal = {arXiv preprint arXiv:2404.01954},
 title = {Hyperclova x technical report},
 year = {2024}
}

@article{hoffmann2022training,
 author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
 journal = {arXiv preprint arXiv:2203.15556},
 title = {Training compute-optimal large language models},
 year = {2022}
}

@article{jacobs1991adaptive,
 author = {Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
 journal = {Neural computation},
 number = {1},
 pages = {79--87},
 publisher = {MIT Press},
 title = {Adaptive mixtures of local experts},
 volume = {3},
 year = {1991}
}

@article{jordan1994hierarchical,
 author = {Jordan, Michael I and Jacobs, Robert A},
 journal = {Neural computation},
 number = {2},
 pages = {181--214},
 publisher = {MIT Press},
 title = {Hierarchical mixtures of experts and the EM algorithm},
 volume = {6},
 year = {1994}
}

@article{collobert2001parallel,
 author = {Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
 journal = {Advances in Neural Information Processing Systems},
 title = {A parallel mixture of SVMs for very large scale problems},
 volume = {14},
 year = {2001}
}

@article{rasmussen2001infinite,
 author = {Rasmussen, Carl and Ghahramani, Zoubin},
 journal = {Advances in neural information processing systems},
 title = {Infinite mixtures of Gaussian process experts},
 volume = {14},
 year = {2001}
}

@article{shahbaba2009nonlinear,
 author = {Shahbaba, Babak and Neal, Radford},
 journal = {Journal of Machine Learning Research},
 number = {8},
 title = {Nonlinear models using Dirichlet process mixtures.},
 volume = {10},
 year = {2009}
}

@article{eigen2013learning,
 author = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
 journal = {arXiv preprint arXiv:1312.4314},
 title = {Learning factored representations in a deep mixture of experts},
 year = {2013}
}

@article{theis2015generative,
 author = {Theis, Lucas and Bethge, Matthias},
 journal = {Advances in neural information processing systems},
 title = {Generative image modeling using spatial lstms},
 volume = {28},
 year = {2015}
}

@inproceedings{deisenroth2015distributed,
 author = {Deisenroth, Marc and Ng, Jun Wei},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {1481--1490},
 title = {Distributed gaussian processes},
 year = {2015}
}

@inproceedings{aljundi2017expert,
 author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
 booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
 pages = {3366--3375},
 title = {Expert gate: Lifelong learning with a network of experts},
 year = {2017}
}

@article{shazeer2017outrageously,
 author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
 journal = {arXiv preprint arXiv:1701.06538},
 title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
 year = {2017}
}

@article{lepikhin2020gshard,
 author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
 journal = {arXiv preprint arXiv:2006.16668},
 shorttitle = {Gshard},
 title = {Gshard: Scaling giant models with conditional computation and automatic sharding},
 year = {2020}
}

@article{jiang2024mixtral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
 journal = {arXiv preprint arXiv:2401.04088},
 title = {Mixtral of experts},
 year = {2024}
}
@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 journal = {Advances in neural information processing systems},
 title = {Attention is all you need},
 volume = {30},
 year = {2017}
}

@article{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 journal = {Advances in neural information processing systems},
 pages = {1877--1901},
 title = {Language models are few-shot learners},
 volume = {33},
 year = {2020}
}

@article{chowdhery2023palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {Journal of Machine Learning Research},
 number = {240},
 pages = {1--113},
 shorttitle = {Palm},
 title = {Palm: Scaling language modeling with pathways},
 volume = {24},
 year = {2023}
}

@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {arXiv preprint arXiv:2303.08774},
 title = {Gpt-4 technical report},
 year = {2023}
}

@article{jiang2024survey,
 author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
 journal = {arXiv preprint arXiv:2406.00515},
 title = {A survey on large language models for code generation},
 year = {2024}
}

@article{riquelme2021scaling,
 author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
 journal = {Advances in Neural Information Processing Systems},
 pages = {8583--8595},
 title = {Scaling vision with sparse mixture of experts},
 volume = {34},
 year = {2021}
}

@inproceedings{liu2021swin,
 author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
 booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
 pages = {10012--10022},
 title = {Swin transformer: Hierarchical vision transformer using shifted windows},
 year = {2021}
}

@article{lu2019vilbert,
 author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
 journal = {Advances in neural information processing systems},
 shorttitle = {Vilbert},
 title = {Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
 volume = {32},
 year = {2019}
}

@article{zhou2022learning,
 author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
 journal = {International Journal of Computer Vision},
 number = {9},
 pages = {2337--2348},
 publisher = {Springer},
 title = {Learning to prompt for vision-language models},
 volume = {130},
 year = {2022}
}

@article{zhu2023minigpt,
 author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2304.10592},
 shorttitle = {Minigpt-4},
 title = {Minigpt-4: Enhancing vision-language understanding with advanced large language models},
 year = {2023}
}

@article{kaplan2020scaling,
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 journal = {arXiv preprint arXiv:2001.08361},
 title = {Scaling laws for neural language models},
 year = {2020}
}

@article{wei2022emergent,
 author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
 journal = {arXiv preprint arXiv:2206.07682},
 title = {Emergent abilities of large language models},
 year = {2022}
}

@article{yoo2024hyperclova,
 author = {Yoo, Kang Min and Han, Jaegeun and In, Sookyo and Jeon, Heewon and Jeong, Jisu and Kang, Jaewook and Kim, Hyunwook and Kim, Kyung-Min and Kim, Munhyong and Kim, Sungju and others},
 journal = {arXiv preprint arXiv:2404.01954},
 title = {Hyperclova x technical report},
 year = {2024}
}

@article{hoffmann2022training,
 author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
 journal = {arXiv preprint arXiv:2203.15556},
 title = {Training compute-optimal large language models},
 year = {2022}
}

@article{jacobs1991adaptive,
 author = {Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
 journal = {Neural computation},
 number = {1},
 pages = {79--87},
 publisher = {MIT Press},
 title = {Adaptive mixtures of local experts},
 volume = {3},
 year = {1991}
}

@article{jordan1994hierarchical,
 author = {Jordan, Michael I and Jacobs, Robert A},
 journal = {Neural computation},
 number = {2},
 pages = {181--214},
 publisher = {MIT Press},
 title = {Hierarchical mixtures of experts and the EM algorithm},
 volume = {6},
 year = {1994}
}

@article{collobert2001parallel,
 author = {Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
 journal = {Advances in Neural Information Processing Systems},
 title = {A parallel mixture of SVMs for very large scale problems},
 volume = {14},
 year = {2001}
}

@article{rasmussen2001infinite,
 author = {Rasmussen, Carl and Ghahramani, Zoubin},
 journal = {Advances in neural information processing systems},
 title = {Infinite mixtures of Gaussian process experts},
 volume = {14},
 year = {2001}
}
@article{shahbaba2009nonlinear,
 author = {Shahbaba, Babak and Neal, Radford},
 journal = {Journal of Machine Learning Research},
 number = {8},
 title = {Nonlinear models using Dirichlet process mixtures.},
 volume = {10},
 year = {2009}
}

@article{eigen2013learning,
 author = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
 journal = {arXiv preprint arXiv:1312.4314},
 title = {Learning factored representations in a deep mixture of experts},
 year = {2013}
}

@article{theis2015generative,
 author = {Theis, Lucas and Bethge, Matthias},
 journal = {Advances in neural information processing systems},
 title = {Generative image modeling using spatial lstms},
 volume = {28},
 year = {2015}
}

@inproceedings{deisenroth2015distributed,
 author = {Deisenroth, Marc and Ng, Jun Wei},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {1481--1490},
 title = {Distributed gaussian processes},
 year = {2015}
}

@inproceedings{aljundi2017expert,
 author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
 booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
 pages = {3366--3375},
 title = {Expert gate: Lifelong learning with a network of experts},
 year = {2017}
}

@article{shazeer2017outrageously,
 author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
 journal = {arXiv preprint arXiv:1701.06538},
 title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
 year = {2017}
}

@article{lepikhin2020gshard,
 author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
 journal = {arXiv preprint arXiv:2006.16668},
 shorttitle = {Gshard},
 title = {Gshard: Scaling giant models with conditional computation and automatic sharding},
 year = {2020}
}

@article{jiang2024mixtral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
 journal = {arXiv preprint arXiv:2401.04088},
 title = {Mixtral of experts},
 year = {2024}
}

@article{liu2024deepseek,
 author = {Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
 journal = {arXiv preprint arXiv:2405.04434},
 shorttitle = {Deepseek-v2},
 title = {Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
 year = {2024}
}

@article{yuksel2012twenty,
 author = {Yuksel, Seniha Esen and Wilson, Joseph N and Gader, Paul D},
 journal = {IEEE transactions on neural networks and learning systems},
 number = {8},
 pages = {1177--1193},
 publisher = {IEEE},
 title = {Twenty years of mixture of experts},
 volume = {23},
 year = {2012}
}

@article{fedus2022review,
 author = {Fedus, William and Dean, Jeff and Zoph, Barret},
 journal = {arXiv preprint arXiv:2209.01667},
 title = {A review of sparse expert models in deep learning},
 year = {2022}
}

@inproceedings{du2022glam,
 author = {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {5547--5569},
 shorttitle = {Glam},
 title = {Glam: Efficient scaling of language models with mixture-of-experts},
 year = {2022}
}

@article{fedus2022switch,
 author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
 journal = {Journal of Machine Learning Research},
 number = {120},
 pages = {1--39},
 title = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
 volume = {23},
 year = {2022}
}

@article{zoph2022st,
 author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
 journal = {arXiv preprint arXiv:2202.08906},
 shorttitle = {St-moe},
 title = {St-moe: Designing stable and transferable sparse expert models},
 year = {2022}
}

@article{xue2024openmoe,
 author = {Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei and Zhou, Wangchunshu and You, Yang},
 journal = {arXiv preprint arXiv:2402.01739},
 shorttitle = {Openmoe},
 title = {Openmoe: An early effort on open mixture-of-experts language models},
 year = {2024}
}

@article{puigcerver2023sparse,
 author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
 journal = {arXiv preprint arXiv:2308.00951},
 title = {From sparse to soft mixtures of experts},
 year = {2023}
}

@article{muqeeth2023soft,
 author = {Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin},
 journal = {arXiv preprint arXiv:2306.03745},
 title = {Soft merging of experts with adaptive routing},
 year = {2023}
}

@article{zhong2024lory,
 author = {Zhong, Zexuan and Xia, Mengzhou and Chen, Danqi and Lewis, Mike},
 journal = {arXiv preprint arXiv:2405.03133},
 shorttitle = {Lory},
 title = {Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training},
 year = {2024}
}

@article{zadouri2023pushing,
 author = {Zadouri, Ted and {\"U}st{\"u}n, Ahmet and Ahmadian, Arash and Ermi{\c{s}}, Beyza and Locatelli, Acyr and Hooker, Sara},
 journal = {arXiv preprint arXiv:2309.05444},
 title = {Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning},
 year = {2023}
}

@inproceedings{wu2024omni,
 author = {Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {14205--14215},
 shorttitle = {Omni-smola},
 title = {Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts},
 year = {2024}
}

@inproceedings{wang2022adamix,
 author = {Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Hassan, Ahmed and Gao, Jianfeng},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {5744--5760},
 shorttitle = {Adamix},
 title = {Adamix: Mixture-of-adaptations for parameter-efficient model tuning},
 year = {2022}
}

@article{dou2023loramoe,
 author = {Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
 journal = {arXiv preprint arXiv:2312.09979},
 number = {7},
 shorttitle = {Loramoe},
 title = {Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment},
 volume = {4},
 year = {2023}
}

@article{gou2023mixture,
 author = {Gou, Yunhao and Liu, Zhili and Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Aoxue and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
 journal = {arXiv preprint arXiv:2312.12379},
 title = {Mixture of cluster-conditional lora experts for vision-language instruction tuning},
 year = {2023}
}

@article{luo2024moelora,
 author = {Luo, Tongxu and Lei, Jiahe and Lei, Fangyu and Liu, Weihao and He, Shizhu and Zhao, Jun and Liu, Kang},
 journal = {arXiv preprint arXiv:2402.12851},
 shorttitle = {Moelora},
 title = {Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models},
 year = {2024}
}

@article{wu2024mixture,
 author = {Wu, Xun and Huang, Shaohan and Wei, Furu},
 journal = {arXiv preprint arXiv:2404.13628},
 title = {Mixture of lora experts},
 year = {2024}
}
@article{komatsuzaki2022sparse,
 author = {Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil},
 journal = {arXiv preprint arXiv:2212.05055},
 title = {Sparse upcycling: Training mixture-of-experts from dense checkpoints},
 year = {2022}
}

@inproceedings{zhang2022moefication,
 author = {Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
 pages = {877--890},
 shorttitle = {Moefication},
 title = {Moefication: Transformer feed-forward layers are mixtures of experts},
 year = {2022}
}

@article{zhu2024llama,
 author = {Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu},
 journal = {arXiv preprint arXiv:2406.16554},
 shorttitle = {Llama-moe},
 title = {Llama-moe: Building mixture-of-experts from llama with continual pre-training},
 year = {2024}
}

@article{xue2022one,
 author = {Xue, Fuzhao and He, Xiaoxin and Ren, Xiaozhe and Lou, Yuxuan and You, Yang},
 journal = {arXiv preprint arXiv:2201.10890},
 title = {One student knows all experts know: From sparse to dense},
 year = {2022}
}

@article{chen2022task,
 author = {Chen, Tianyu and Huang, Shaohan and Xie, Yuan and Jiao, Binxing and Jiang, Daxin and Zhou, Haoyi and Li, Jianxin and Wei, Furu},
 journal = {arXiv preprint arXiv:2206.00277},
 title = {Task-specific expert pruning for sparse mixture-of-experts},
 year = {2022}
}

@article{sukhbaatar2024branch,
 author = {Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozi{\`e}re, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and others},
 journal = {arXiv preprint arXiv:2403.07816},
 shorttitle = {Branch-train-mix},
 title = {Branch-train-mix: Mixing expert llms into a mixture-of-experts llm},
 year = {2024}
}

@inproceedings{chen2023lifelong,
 author = {Chen, Wuyang and Zhou, Yanqi and Du, Nan and Huang, Yanping and Laudon, James and Chen, Zhifeng and Cui, Claire},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {5383--5395},
 title = {Lifelong language pretraining with distribution-specialized experts},
 year = {2023}
}

@article{raposo2024mixture,
 author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
 journal = {arXiv preprint arXiv:2404.02258},
 shorttitle = {Mixture-of-depths},
 title = {Mixture-of-depths: Dynamically allocating compute in transformer-based language models},
 year = {2024}
}

@inproceedings{xue2022go,
 author = {Xue, Fuzhao and Shi, Ziji and Wei, Futao and Lou, Yuxuan and Liu, Yong and You, Yang},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {8},
 pages = {8779--8787},
 title = {Go wider instead of deeper},
 volume = {36},
 year = {2022}
}

@article{tan2023sparse,
 author = {Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
 journal = {arXiv preprint arXiv:2310.07096},
 title = {Sparse universal transformer},
 year = {2023}
}

@inproceedings{choi2023smop,
 author = {Choi, Joon-Young and Kim, Junho and Park, Jun-Hyung and Mok, Wing-Lam and Lee, SangKeun},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 pages = {14306--14316},
 shorttitle = {Smop},
 title = {Smop: Towards efficient and effective prompt tuning with sparse mixture-of-prompts},
 year = {2023}
}

@inproceedings{ma2018modeling,
 author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H},
 booktitle = {Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
 pages = {1930--1939},
 title = {Modeling task relationships in multi-task learning with multi-gate mixture-of-experts},
 year = {2018}
}

@article{nie2021evomoe,
 author = {Nie, Xiaonan and Miao, Xupeng and Cao, Shijie and Ma, Lingxiao and Liu, Qibin and Xue, Jilong and Miao, Youshan and Liu, Yi and Yang, Zhi and Cui, Bin},
 journal = {arXiv preprint arXiv:2112.14397},
 shorttitle = {Evomoe},
 title = {Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate},
 year = {2021}
}

@article{pan2024dense,
 author = {Pan, Bowen and Shen, Yikang and Liu, Haokun and Mishra, Mayank and Zhang, Gaoyuan and Oliva, Aude and Raffel, Colin and Panda, Rameswar},
 journal = {arXiv preprint arXiv:2404.05567},
 title = {Dense training, sparse inference: Rethinking training of mixture-of-experts language models},
 year = {2024}
}

@inproceedings{clark2022unified,
 author = {Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {4057--4086},
 title = {Unified scaling laws for routed language models},
 year = {2022}
}
@misc{snowflake2024research,
 author = {Snowflake, AI},
 title = {Research. Snowflake Arctic: The best LLM for enterprise AIâ€”efficiently intelligent, truly open},
 year = {2024}
}

@article{antoniak2024mixture,
 author = {Antoniak, Szymon and Krutul, Micha{\l} and Pi{\'o}ro, Maciej and Krajewski, Jakub and Ludziejewski, Jan and Ciebiera, Kamil and Kr{\'o}l, Krystian and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Cygan, Marek and Jaszczur, Sebastian},
 journal = {Advances in Neural Information Processing Systems},
 pages = {103873--103896},
 title = {Mixture of tokens: Continuous moe through cross-example aggregation},
 volume = {37},
 year = {2024}
}

@inproceedings{rajbhandari2022deepspeed,
 author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {18332--18346},
 shorttitle = {Deepspeed-moe},
 title = {Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
 year = {2022}
}

@article{wei2024skywork,
 author = {Wei, Tianwen and Zhu, Bo and Zhao, Liang and Cheng, Cheng and Li, Biye and L{\"u}, Weiwei and Cheng, Peng and Zhang, Jianhao and Zhang, Xiaoyu and Zeng, Liang and others},
 journal = {arXiv preprint arXiv:2406.06563},
 shorttitle = {Skywork-moe},
 title = {Skywork-moe: A deep dive into training techniques for mixture-of-experts language models},
 year = {2024}
}

@article{lieber2024jamba,
 author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
 journal = {arXiv preprint arXiv:2403.19887},
 shorttitle = {Jamba},
 title = {Jamba: A hybrid transformer-mamba language model},
 year = {2024}
}

@article{dai2024deepseekmoe,
 author = {Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Yu and others},
 journal = {arXiv preprint arXiv:2401.06066},
 shorttitle = {Deepseekmoe},
 title = {Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
 year = {2024}
}

@article{yang2021m6,
 author = {Yang, An and Lin, Junyang and Men, Rui and Zhou, Chang and Jiang, Le and Jia, Xianyan and Wang, Ang and Zhang, Jie and Wang, Jiamang and Li, Yong and others},
 journal = {arXiv preprint arXiv:2105.15082},
 shorttitle = {M6-t},
 title = {M6-t: Exploring sparse expert models and beyond},
 year = {2021}
}

@inproceedings{chen2023mod,
 author = {Chen, Zitian and Shen, Yikang and Ding, Mingyu and Chen, Zhenfang and Zhao, Hengshuang and Learned-Miller, Erik G and Gan, Chuang},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {11828--11837},
 shorttitle = {Mod-squad},
 title = {Mod-squad: Designing mixtures of experts as modular multi-task learners},
 year = {2023}
}

@article{wu2024yuan,
 author = {Wu, Shaohua and Luo, Jiangang and Chen, Xi and Li, Lingjun and Zhao, Xudong and Yu, Tong and Wang, Chao and Wang, Yue and Wang, Fei and Qiao, Weixu and others},
 journal = {arXiv preprint arXiv:2405.17976},
 title = {Yuan 2.0-m32: Mixture of experts with attention router},
 year = {2024}
}

@inproceedings{lewis2021base,
 author = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {6265--6274},
 title = {Base layers: Simplifying training of large, sparse models},
 year = {2021}
}

@article{hazimeh2021dselect,
 author = {Hazimeh, Hussein and Zhao, Zhe and Chowdhery, Aakanksha and Sathiamoorthy, Maheswaran and Chen, Yihua and Mazumder, Rahul and Hong, Lichan and Chi, Ed},
 journal = {Advances in Neural Information Processing Systems},
 pages = {29335--29347},
 shorttitle = {Dselect-k},
 title = {Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning},
 volume = {34},
 year = {2021}
}
@article{kim2021scalable,
 author = {Kim, Young Jin and Awan, Ammar Ahmad and Muzio, Alexandre and Salinas, Andres Felipe Cruz and Lu, Liyang and Hendy, Amr and Rajbhandari, Samyam and He, Yuxiong and Awadalla, Hany Hassan},
 journal = {arXiv preprint arXiv:2109.10465},
 title = {Scalable and efficient moe training for multitask multilingual models},
 year = {2021}
}

@article{kudugunta2021beyond,
 author = {Kudugunta, Sneha and Huang, Yanping and Bapna, Ankur and Krikun, Maxim and Lepikhin, Dmitry and Luong, Minh-Thang and Firat, Orhan},
 journal = {arXiv preprint arXiv:2110.03742},
 title = {Beyond distillation: Task-level mixture-of-experts for efficient inference},
 year = {2021}
}

@article{costa2022no,
 author = {Costa-Juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
 journal = {arXiv preprint arXiv:2207.04672},
 title = {No language left behind: Scaling human-centered machine translation},
 year = {2022}
}

@article{chi2022representation,
 author = {Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Mao, Xian-Ling and others},
 journal = {Advances in Neural Information Processing Systems},
 pages = {34600--34613},
 title = {On the representation collapse of sparse mixture of experts},
 volume = {35},
 year = {2022}
}

@article{zhu2022uni,
 author = {Zhu, Jinguo and Zhu, Xizhou and Wang, Wenhai and Wang, Xiaohua and Li, Hongsheng and Wang, Xiaogang and Dai, Jifeng},
 journal = {Advances in Neural Information Processing Systems},
 pages = {2664--2678},
 shorttitle = {Uni-perceiver-moe},
 title = {Uni-perceiver-moe: Learning sparse generalist models with conditional moes},
 volume = {35},
 year = {2022}
}

@article{dai2022stablemoe,
 author = {Dai, Damai and Dong, Li and Ma, Shuming and Zheng, Bo and Sui, Zhifang and Chang, Baobao and Wei, Furu},
 journal = {arXiv preprint arXiv:2204.08396},
 shorttitle = {Stablemoe},
 title = {Stablemoe: Stable routing strategy for mixture of experts},
 year = {2022}
}

@article{roller2021hash,
 author = {Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
 journal = {advances in neural information processing systems},
 pages = {17555--17566},
 title = {Hash layers for large sparse models},
 volume = {34},
 year = {2021}
}

@article{zuo2021taming,
 author = {Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Kim, Young Jin and Hassan, Hany and Zhang, Ruofei and Zhao, Tuo and Gao, Jianfeng},
 journal = {arXiv preprint arXiv:2110.04260},
 title = {Taming sparsely activated transformer with stochastic experts},
 year = {2021}
}

@inproceedings{gururangan2022demix,
 author = {Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A and Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {5557--5576},
 title = {Demix layers: Disentangling domains for modular language modeling},
 year = {2022}
}

@article{fan2021beyond,
 author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
 journal = {Journal of Machine Learning Research},
 number = {107},
 pages = {1--48},
 title = {Beyond english-centric multilingual machine translation},
 volume = {22},
 year = {2021}
}

@article{zhou2022mixture,
 author = {Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
 journal = {Advances in Neural Information Processing Systems},
 pages = {7103--7114},
 title = {Mixture-of-experts with expert choice routing},
 volume = {35},
 year = {2022}
}

@inproceedings{zhou2023brainformers,
 author = {Zhou, Yanqi and Du, Nan and Huang, Yanping and Peng, Daiyi and Lan, Chang and Huang, Da and Shakeri, Siamak and So, David and Dai, Andrew M and Lu, Yifeng and others},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {42531--42542},
 shorttitle = {Brainformers},
 title = {Brainformers: Trading simplicity for efficiency},
 year = {2023}
}

@article{zhang2022mixture,
 author = {Zhang, Xiaofeng and Shen, Yikang and Huang, Zeyu and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
 journal = {arXiv preprint arXiv:2210.05144},
 title = {Mixture of attention heads: Selecting attention heads per token},
 year = {2022}
}

@article{shen2024jetmoe,
 author = {Shen, Yikang and Guo, Zhen and Cai, Tianle and Qin, Zengyi},
 journal = {arXiv preprint arXiv:2404.07413},
 shorttitle = {Jetmoe},
 title = {Jetmoe: Reaching llama2 performance with 0.1 m dollars},
 year = {2024}
}

@article{artetxe2021efficient,
 author = {Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
 journal = {arXiv preprint arXiv:2112.10684},
 title = {Efficient large scale language modeling with mixtures of experts},
 year = {2021}
}

@article{lin2024moe,
 author = {Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Huang, Jinfa and Zhang, Junwu and Pang, Yatian and Ning, Munan and others},
 journal = {arXiv preprint arXiv:2401.15947},
 shorttitle = {Moe-llava},
 title = {Moe-llava: Mixture of experts for large vision-language models},
 year = {2024}
}
@article{team2024gemma,
 author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
 journal = {arXiv preprint arXiv:2408.00118},
 title = {Gemma 2: Improving open language models at a practical size},
 year = {2024}
}

@article{shen2023moduleformer,
 author = {Shen, Yikang and Zhang, Zheyu and Cao, Tianyou and Tan, Shawn and Chen, Zhenfang and Gan, Chuang},
 journal = {arXiv preprint arXiv:2306.04640},
 shorttitle = {ModuleFormer},
 title = {ModuleFormer: Modularity Emerges from Mixture-of-Experts},
 year = {2023}
}

@article{ren2023pangu,
 author = {Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and others},
 journal = {arXiv preprint arXiv:2303.10845},
 shorttitle = {Pangu-$\{$$\backslash$Sigma$\}$},
 title = {Pangu-$\{$$\backslash$Sigma$\}$: Towards trillion parameter language model with sparse heterogeneous computing},
 year = {2023}
}

@inproceedings{mao2025hope,
 author = {Mao, Po-Yuan and Nguyen, Tien Hoang and Buntine, Wray and Bennamoun, Mohammed and others},
 booktitle = {2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
 organization = {IEEE},
 pages = {1101--1110},
 shorttitle = {HOPE},
 title = {HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts},
 year = {2025}
}

@article{diao2023mixture,
 author = {Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
 journal = {arXiv preprint arXiv:2306.05406},
 shorttitle = {Mixture-of-domain-adapters},
 title = {Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories},
 year = {2023}
}

@article{chen2024llava,
 author = {Chen, Shaoxiang and Jie, Zequn and Ma, Lin},
 journal = {arXiv preprint arXiv:2401.16160},
 shorttitle = {Llava-mole},
 title = {Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms},
 year = {2024}
}

@article{zhu2023sira,
 author = {Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others},
 journal = {arXiv preprint arXiv:2311.09179},
 shorttitle = {Sira},
 title = {Sira: Sparse mixture of low rank adaptation},
 year = {2023}
}

@inproceedings{mao2022unipelt,
 author = {Mao, Yuning and Mathias, Lambert and Hou, Rui and Almahairi, Amjad and Ma, Hao and Han, Jiawei and Yih, Scott and Khabsa, Madian},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {6253--6264},
 shorttitle = {Unipelt},
 title = {Unipelt: A unified framework for parameter-efficient language model tuning},
 year = {2022}
}

@article{gao2024higher,
 author = {Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS},
 journal = {arXiv preprint arXiv:2402.08562},
 title = {Higher layers need more lora experts},
 year = {2024}
}

@article{wu2022residual,
 author = {Wu, Lemeng and Liu, Mengchen and Chen, Yinpeng and Chen, Dongdong and Dai, Xiyang and Yuan, Lu},
 journal = {arXiv preprint arXiv:2204.09636},
 title = {Residual mixture of experts},
 year = {2022}
}

@inproceedings{dua2022tricks,
 author = {Dua, Dheeru and Bhosale, Shruti and Goswami, Vedanuj and Cross, James and Lewis, Mike and Fan, Angela},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {3340--3345},
 title = {Tricks for training sparse translation models},
 year = {2022}
}

@article{huang2023experts,
 author = {Huang, Yongqi and Ye, Peng and Huang, Xiaoshui and Li, Sheng and Chen, Tao and He, Tong and Ouyang, Wanli},
 journal = {arXiv preprint arXiv:2308.06093},
 title = {Experts weights averaging: A new general training scheme for vision transformers},
 year = {2023}
}

@article{li2022branch,
 author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
 journal = {arXiv preprint arXiv:2208.03306},
 shorttitle = {Branch-train-merge},
 title = {Branch-train-merge: Embarrassingly parallel training of expert language models},
 year = {2022}
}

@article{wang2023fusing,
 author = {Wang, Hongyi and Polo, Felipe Maia and Sun, Yuekai and Kundu, Souvik and Xing, Eric and Yurochkin, Mikhail},
 journal = {arXiv preprint arXiv:2310.01542},
 title = {Fusing models with complementary expertise},
 year = {2023}
}

@article{he2021fastmoe,
 author = {He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
 journal = {arXiv preprint arXiv:2103.13262},
 shorttitle = {Fastmoe},
 title = {Fastmoe: A fast mixture-of-expert training system},
 year = {2021}
}

@article{hwang2023tutel,
 author = {Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {269--287},
 shorttitle = {Tutel},
 title = {Tutel: Adaptive mixture-of-experts at scale},
 volume = {5},
 year = {2023}
}

@inproceedings{he2022fastermoe,
 author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
 booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 pages = {120--134},
 shorttitle = {Fastermoe},
 title = {Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models},
 year = {2022}
}

@inproceedings{singh2023hybrid,
 author = {Singh, Siddharth and Ruwase, Olatunji and Awan, Ammar Ahmad and Rajbhandari, Samyam and He, Yuxiong and Bhatele, Abhinav},
 booktitle = {Proceedings of the 37th International Conference on Supercomputing},
 pages = {203--214},
 title = {A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training},
 year = {2023}
}

@article{nie2022hetumoe,
 author = {Nie, Xiaonan and Zhao, Pinxue and Miao, Xupeng and Zhao, Tong and Cui, Bin},
 journal = {arXiv preprint arXiv:2203.14685},
 shorttitle = {HetuMoE},
 title = {HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system},
 year = {2022}
}

@article{nie2023flexmoe,
 author = {Nie, Xiaonan and Miao, Xupeng and Wang, Zilong and Yang, Zichao and Xue, Jilong and Ma, Lingxiao and Cao, Gang and Cui, Bin},
 journal = {Proceedings of the ACM on Management of Data},
 number = {1},
 pages = {1--19},
 publisher = {ACM New York, NY, USA},
 shorttitle = {Flexmoe},
 title = {Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement},
 volume = {1},
 year = {2023}
}

@inproceedings{zhai2023smartmoe,
 author = {Zhai, Mingshu and He, Jiaao and Ma, Zixuan and Zong, Zan and Zhang, Runqing and Zhai, Jidong},
 booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
 pages = {961--975},
 shorttitle = {$\{$SmartMoE$\}$},
 title = {$\{$SmartMoE$\}$: Efficiently training $\{$Sparsely-Activated$\}$ models through combining offline and online parallelization},
 year = {2023}
}

@article{gale2023megablocks,
 author = {Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {288--304},
 shorttitle = {Megablocks},
 title = {Megablocks: Efficient sparse training with mixture-of-experts},
 volume = {5},
 year = {2023}
}

@article{tan2024scattered,
 author = {Tan, Shawn and Shen, Yikang and Panda, Rameswar and Courville, Aaron},
 journal = {arXiv preprint arXiv:2403.08245},
 title = {Scattered mixture-of-experts implementation},
 year = {2024}
}

@inproceedings{zheng2023pit,
 author = {Zheng, Ningxin and Jiang, Huiqiang and Zhang, Quanlu and Han, Zhenhua and Ma, Lingxiao and Yang, Yuqing and Yang, Fan and Zhang, Chengruidong and Qiu, Lili and Yang, Mao and others},
 booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
 pages = {331--347},
 shorttitle = {Pit},
 title = {Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation},
 year = {2023}
}

@inproceedings{yao2024exploiting,
 author = {Yao, Jinghan and Anthony, Quentin and Shafi, Aamir and Subramoni, Hari and Panda, Dhabaleswar K DK},
 booktitle = {2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
 organization = {IEEE},
 pages = {915--925},
 title = {Exploiting inter-layer expert affinity for accelerating mixture-of-experts model inference},
 year = {2024}
}

@article{chen2022ta,
 author = {Chen, Chang and Li, Min and Wu, Zhihua and Yu, Dianhai and Yang, Chao},
 journal = {Advances in Neural Information Processing Systems},
 pages = {22173--22186},
 shorttitle = {Ta-moe},
 title = {Ta-moe: Topology-aware large scale mixture-of-expert training},
 volume = {35},
 year = {2022}
}

@article{zhang2024mpmoe,
 author = {Zhang, Zheng and Xia, Yaqi and Wang, Hulin and Yang, Donglin and Hu, Chuang and Zhou, Xiaobo and Cheng, Dazhao},
 journal = {IEEE Transactions on Parallel and Distributed Systems},
 number = {6},
 pages = {998--1011},
 publisher = {IEEE},
 shorttitle = {Mpmoe},
 title = {Mpmoe: Memory efficient moe for pre-trained models with adaptive pipeline parallelism},
 volume = {35},
 year = {2024}
}

@article{jiang2024lancet,
 author = {Jiang, Chenyu and Tian, Ye and Jia, Zhen and Zheng, Shuai and Wu, Chuan and Wang, Yida},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {74--86},
 shorttitle = {Lancet},
 title = {Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping},
 volume = {6},
 year = {2024}
}

@article{cai2024shortcut,
 author = {Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
 journal = {arXiv preprint arXiv:2404.05019},
 title = {Shortcut-connected expert parallelism for accelerating mixture-of-experts},
 year = {2024}
}

@inproceedings{hwang2024pre,
 author = {Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao},
 booktitle = {2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
 organization = {IEEE},
 pages = {1018--1031},
 title = {Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference},
 year = {2024}
}

@inproceedings{zhang2023robust,
 author = {Zhang, Yihua and Cai, Ruisi and Chen, Tianlong and Zhang, Guanhua and Zhang, Huan and Chen, Pin-Yu and Chang, Shiyu and Wang, Zhangyang and Liu, Sijia},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
 pages = {90--101},
 title = {Robust mixture-of-expert training for convolutional neural networks},
 year = {2023}
}

@inproceedings{chowdhury2023patch,
 author = {Chowdhury, Mohammed Nowaz Rabbani and Zhang, Shuai and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {6074--6114},
 title = {Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks},
 year = {2023}
}
@article{zhang2024t,
 author = {Zhang, Rongyu and Liu, Yijiang and Yang, Huanrui and Zheng, Shenli and Wang, Dan and Du, Yuan and Du, Li and Zhang, Shanghang},
 journal = {arXiv preprint arXiv:2404.08985},
 shorttitle = {T-REX},
 title = {T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning},
 year = {2024}
}

@inproceedings{tang2020progressive,
 author = {Tang, Hongyan and Liu, Junning and Zhao, Ming and Gong, Xudong},
 booktitle = {Proceedings of the 14th ACM conference on recommender systems},
 pages = {269--278},
 title = {Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations},
 year = {2020}
}

@inproceedings{jiang2023adamct,
 author = {Jiang, Juyong and Zhang, Peiyan and Luo, Yingtao and Li, Chaozhuo and Kim, Jae Boum and Zhang, Kai and Wang, Senzhang and Xie, Xing and Kim, Sunghun},
 booktitle = {Proceedings of the 32nd ACM international conference on information and knowledge management},
 pages = {976--986},
 shorttitle = {AdaMCT},
 title = {AdaMCT: adaptive mixture of CNN-transformer for sequential recommendation},
 year = {2023}
}
