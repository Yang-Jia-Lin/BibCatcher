- [1: Attention is all you need](zotero://select/items/@vaswani2017attention)
- [2: Language models are few-shot learners](zotero://select/items/@brown2020language)
- [3: Palm: Scaling language modeling with pathways](zotero://select/items/@chowdhery2023palm)
- [4: Gpt-4 technical report](zotero://select/items/@achiam2023gpt)
- [5: A survey on large language models for code generation](zotero://select/items/@jiang2024survey)
- [6: Scaling vision with sparse mixture of experts](zotero://select/items/@riquelme2021scaling)
- [7: Swin transformer: Hierarchical vision transformer using shifted windows](zotero://select/items/@liu2021swin)
- [8: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](zotero://select/items/@lu2019vilbert)
- [9: Learning to prompt for vision-language models](zotero://select/items/@zhou2022learning)
- [10: Minigpt-4: Enhancing vision-language understanding with advanced large language models](zotero://select/items/@zhu2023minigpt)
- [11: Scaling laws for neural language models](zotero://select/items/@kaplan2020scaling)
- [12: Emergent abilities of large language models](zotero://select/items/@wei2022emergent)
- [13: Hyperclova x technical report](zotero://select/items/@yoo2024hyperclova)
- [14: Training compute-optimal large language models](zotero://select/items/@hoffmann2022training)
- [15: Adaptive mixtures of local experts](zotero://select/items/@jacobs1991adaptive)
- [16: Hierarchical mixtures of experts and the EM algorithm](zotero://select/items/@jordan1994hierarchical)
- [17: A parallel mixture of SVMs for very large scale problems](zotero://select/items/@collobert2001parallel)
- [18: Infinite mixtures of Gaussian process experts](zotero://select/items/@rasmussen2001infinite)
- [19: Nonlinear models using Dirichlet process mixtures.](zotero://select/items/@shahbaba2009nonlinear)
- [20: Learning factored representations in a deep mixture of experts](zotero://select/items/@eigen2013learning)
- [21: Generative image modeling using spatial lstms](zotero://select/items/@theis2015generative)
- [22: Distributed gaussian processes](zotero://select/items/@deisenroth2015distributed)
- [23: Expert gate: Lifelong learning with a network of experts](zotero://select/items/@aljundi2017expert)
- [24: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer](zotero://select/items/@shazeer2017outrageously)
- [25: Gshard: Scaling giant models with conditional computation and automatic sharding](zotero://select/items/@lepikhin2020gshard)
- [26: Mixtral of experts](zotero://select/items/@jiang2024mixtral)
- [27: Gemma 2: Improving open language models at a practical size](zotero://select/items/@team2024gemma)
- [29: Research. Snowflake Arctic: The best LLM for enterprise AIâ€”efficiently intelligent, truly open](zotero://select/items/@snowflake2024research)
- [30: Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model](zotero://select/items/@liu2024deepseek)
- [31: Twenty years of mixture of experts](zotero://select/items/@yuksel2012twenty)
- [32: A review of sparse expert models in deep learning](zotero://select/items/@fedus2022review)
- [33: Glam: Efficient scaling of language models with mixture-of-experts](zotero://select/items/@du2022glam)
- [34: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity](zotero://select/items/@fedus2022switch)
- [35: St-moe: Designing stable and transferable sparse expert models](zotero://select/items/@zoph2022st)
- [36: Openmoe: An early effort on open mixture-of-experts language models](zotero://select/items/@xue2024openmoe)
- [37: From sparse to soft mixtures of experts](zotero://select/items/@puigcerver2023sparse)
- [38: Soft merging of experts with adaptive routing](zotero://select/items/@muqeeth2023soft)
- [39: Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training](zotero://select/items/@zhong2024lory)
- [40: Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning](zotero://select/items/@zadouri2023pushing)
- [41: Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts](zotero://select/items/@wu2024omni)
- [42: Adamix: Mixture-of-adaptations for parameter-efficient model tuning](zotero://select/items/@wang2022adamix)
- [43: Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment](zotero://select/items/@dou2023loramoe)
- [44: Mixture of cluster-conditional lora experts for vision-language instruction tuning](zotero://select/items/@gou2023mixture)
- [45: Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models](zotero://select/items/@luo2024moelora)
- [46: Mixture of lora experts](zotero://select/items/@wu2024mixture)
- [47: Sparse upcycling: Training mixture-of-experts from dense checkpoints](zotero://select/items/@komatsuzaki2022sparse)
- [48: Moefication: Transformer feed-forward layers are mixtures of experts](zotero://select/items/@zhang2022moefication)
- [49: Llama-moe: Building mixture-of-experts from llama with continual pre-training](zotero://select/items/@zhu2024llama)
- [50: One student knows all experts know: From sparse to dense](zotero://select/items/@xue2022one)
- [51: Task-specific expert pruning for sparse mixture-of-experts](zotero://select/items/@chen2022task)
- [52: Branch-train-mix: Mixing expert llms into a mixture-of-experts llm](zotero://select/items/@sukhbaatar2024branch)
- [53: Lifelong language pretraining with distribution-specialized experts](zotero://select/items/@chen2023lifelong)
- [54: Mixture of tokens: Continuous moe through cross-example aggregation](zotero://select/items/@antoniak2024mixture)
- [55: Mixture-of-depths: Dynamically allocating compute in transformer-based language models](zotero://select/items/@raposo2024mixture)
- [56: Go wider instead of deeper](zotero://select/items/@xue2022go)
- [57: Sparse universal transformer](zotero://select/items/@tan2023sparse)
- [58: Smop: Towards efficient and effective prompt tuning with sparse mixture-of-prompts](zotero://select/items/@choi2023smop)
- [59: Modeling task relationships in multi-task learning with multi-gate mixture-of-experts](zotero://select/items/@ma2018modeling)
- [60: Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate](zotero://select/items/@nie2021evomoe)
- [61: Mixture of lora experts](zotero://select/items/@wu2024mixture)
- [62: Dense training, sparse inference: Rethinking training of mixture-of-experts language models](zotero://select/items/@pan2024dense)
- [63: Unified scaling laws for routed language models](zotero://select/items/@clark2022unified)
- [64: Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale](zotero://select/items/@rajbhandari2022deepspeed)
- [65: Skywork-moe: A deep dive into training techniques for mixture-of-experts language models](zotero://select/items/@wei2024skywork)
- [66: Jamba: A hybrid transformer-mamba language model](zotero://select/items/@lieber2024jamba)
- [67: Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models](zotero://select/items/@dai2024deepseekmoe)
- [68: M6-t: Exploring sparse expert models and beyond](zotero://select/items/@yang2021m6)
- [69: Mod-squad: Designing mixtures of experts as modular multi-task learners](zotero://select/items/@chen2023mod)
- [70: Yuan 2.0-m32: Mixture of experts with attention router](zotero://select/items/@wu2024yuan)
- [71: ModuleFormer: Modularity Emerges from Mixture-of-Experts](zotero://select/items/@shen2023moduleformer)
- [72: Base layers: Simplifying training of large, sparse models](zotero://select/items/@lewis2021base)
- [73: Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning](zotero://select/items/@hazimeh2021dselect)
- [74: Scalable and efficient moe training for multitask multilingual models](zotero://select/items/@kim2021scalable)
- [75: Beyond distillation: Task-level mixture-of-experts for efficient inference](zotero://select/items/@kudugunta2021beyond)
- [76: No language left behind: Scaling human-centered machine translation](zotero://select/items/@costa2022no)
- [77: On the representation collapse of sparse mixture of experts](zotero://select/items/@chi2022representation)
- [78: Uni-perceiver-moe: Learning sparse generalist models with conditional moes](zotero://select/items/@zhu2022uni)
- [79: Stablemoe: Stable routing strategy for mixture of experts](zotero://select/items/@dai2022stablemoe)
- [80: Hash layers for large sparse models](zotero://select/items/@roller2021hash)
- [81: Taming sparsely activated transformer with stochastic experts](zotero://select/items/@zuo2021taming)
- [82: Demix layers: Disentangling domains for modular language modeling](zotero://select/items/@gururangan2022demix)
- [83: Beyond english-centric multilingual machine translation](zotero://select/items/@fan2021beyond)
- [84: Pangu-$\{$$\backslash$Sigma$\}$: Towards trillion parameter language model with sparse heterogeneous computing](zotero://select/items/@ren2023pangu)
- [85: Mixture-of-experts with expert choice routing](zotero://select/items/@zhou2022mixture)
- [86: Brainformers: Trading simplicity for efficiency](zotero://select/items/@zhou2023brainformers)
- [87: HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts](zotero://select/items/@mao2025hope)
- [88: Mixture of attention heads: Selecting attention heads per token](zotero://select/items/@zhang2022mixture)
- [89: Jetmoe: Reaching llama2 performance with 0.1 m dollars](zotero://select/items/@shen2024jetmoe)
- [90: Efficient large scale language modeling with mixtures of experts](zotero://select/items/@artetxe2021efficient)
- [91: Moe-llava: Mixture of experts for large vision-language models](zotero://select/items/@lin2024moe)
- [92: Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories](zotero://select/items/@diao2023mixture)
- [94: Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms](zotero://select/items/@chen2024llava)
- [95: Sira: Sparse mixture of low rank adaptation](zotero://select/items/@zhu2023sira)
- [96: Unipelt: A unified framework for parameter-efficient language model tuning](zotero://select/items/@mao2022unipelt)
- [97: Higher layers need more lora experts](zotero://select/items/@gao2024higher)
- [98: T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning](zotero://select/items/@zhang2024t)
- [99: Residual mixture of experts](zotero://select/items/@wu2022residual)
- [100: Tricks for training sparse translation models](zotero://select/items/@dua2022tricks)
- [101: Experts weights averaging: A new general training scheme for vision transformers](zotero://select/items/@huang2023experts)
- [102: Branch-train-merge: Embarrassingly parallel training of expert language models](zotero://select/items/@li2022branch)
- [103: Fusing models with complementary expertise](zotero://select/items/@wang2023fusing)
- [104: Fastmoe: A fast mixture-of-expert training system](zotero://select/items/@he2021fastmoe)
- [105: Tutel: Adaptive mixture-of-experts at scale](zotero://select/items/@hwang2023tutel)
- [107: Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models](zotero://select/items/@he2022fastermoe)
- [108: A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training](zotero://select/items/@singh2023hybrid)
- [109: HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system](zotero://select/items/@nie2022hetumoe)
- [110: Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement](zotero://select/items/@nie2023flexmoe)
- [111: $\{$SmartMoE$\}$: Efficiently training $\{$Sparsely-Activated$\}$ models through combining offline and online parallelization](zotero://select/items/@zhai2023smartmoe)
- [112: Megablocks: Efficient sparse training with mixture-of-experts](zotero://select/items/@gale2023megablocks)
- [113: Scattered mixture-of-experts implementation](zotero://select/items/@tan2024scattered)
- [114: Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation](zotero://select/items/@zheng2023pit)
- [115: Exploiting inter-layer expert affinity for accelerating mixture-of-experts model inference](zotero://select/items/@yao2024exploiting)
- [116: Ta-moe: Topology-aware large scale mixture-of-expert training](zotero://select/items/@chen2022ta)
- [117: Mpmoe: Memory efficient moe for pre-trained models with adaptive pipeline parallelism](zotero://select/items/@zhang2024mpmoe)
- [118: Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping](zotero://select/items/@jiang2024lancet)
- [119: Shortcut-connected expert parallelism for accelerating mixture-of-experts](zotero://select/items/@cai2024shortcut)
- [120: Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference](zotero://select/items/@hwang2024pre)
- [121: Edgemoe: Fast on-device inference of moe-based large language models](zotero://select/items/@yi2023edgemoe)
- [122: Robust mixture-of-expert training for convolutional neural networks](zotero://select/items/@zhang2023robust)
- [123: Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks](zotero://select/items/@chowdhury2023patch)
- [124: Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations](zotero://select/items/@tang2020progressive)
- [125: AdaMCT: adaptive mixture of CNN-transformer for sequential recommendation](zotero://select/items/@jiang2023adamct)