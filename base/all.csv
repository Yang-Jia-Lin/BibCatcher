citekey,cite_count,title,cite_by
yi2023edgemoe,2,Edgemoe: Fast on-device inference of moe-based large language models,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(7);A Survey on MoE in LLM(121)
zuo2021taming,1,Taming sparsely activated transformer with stochastic experts,A Survey on MoE in LLM(81)
zoph2022st,1,St-moe: Designing stable and transferable sparse expert models,A Survey on MoE in LLM(35)
zhu2024llama,1,Llama-moe: Building mixture-of-experts from llama with continual pre-training,A Survey on MoE in LLM(49)
zhu2023sira,1,Sira: Sparse mixture of low rank adaptation,A Survey on MoE in LLM(95)
zhu2023minigpt,1,Minigpt-4: Enhancing vision-language understanding with advanced large language models,A Survey on MoE in LLM(10)
zhu2022uni,1,Uni-perceiver-moe: Learning sparse generalist models with conditional moes,A Survey on MoE in LLM(78)
zhou2023brainformers,1,Brainformers: Trading simplicity for efficiency,A Survey on MoE in LLM(86)
zhou2022mixture,1,Mixture-of-experts with expert choice routing,A Survey on MoE in LLM(85)
zhou2022learning,1,Learning to prompt for vision-language models,A Survey on MoE in LLM(9)
zhong2024lory,1,Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training,A Survey on MoE in LLM(39)
zheng2025review,1,"A review on edge large language models: Design, execution, and applications",The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(6)
zheng2023pit,1,Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation,A Survey on MoE in LLM(114)
zhang2025communication,1,Communication-efficient distributed on-device llm inference over wireless networks,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(13)
zhang2024t,1,T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning,A Survey on MoE in LLM(98)
zhang2024mpmoe,1,Mpmoe: Memory efficient moe for pre-trained models with adaptive pipeline parallelism,A Survey on MoE in LLM(117)
zhang2024edgeshard,1,Edgeshard: Efficient llm inference via collaborative edge computing,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(1)
zhang2023robust,1,Robust mixture-of-expert training for convolutional neural networks,A Survey on MoE in LLM(122)
zhang2022moefication,1,Moefication: Transformer feed-forward layers are mixtures of experts,A Survey on MoE in LLM(48)
zhang2022mixture,1,Mixture of attention heads: Selecting attention heads per token,A Survey on MoE in LLM(88)
zhai2023smartmoe,1,$\{$SmartMoE$\}$: Efficiently training $\{$Sparsely-Activated$\}$ models through combining offline and online parallelization,A Survey on MoE in LLM(111)
zadouri2023pushing,1,Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning,A Survey on MoE in LLM(40)
yuksel2012twenty,1,Twenty years of mixture of experts,A Survey on MoE in LLM(31)
yoo2024hyperclova,1,Hyperclova x technical report,A Survey on MoE in LLM(13)
ye2024galaxy,1,Galaxy: A resource-efficient collaborative edge ai system for in-situ transformer inference,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(2)
yao2024exploiting,1,Exploiting inter-layer expert affinity for accelerating mixture-of-experts model inference,A Survey on MoE in LLM(115)
yang2021m6,1,M6-t: Exploring sparse expert models and beyond,A Survey on MoE in LLM(68)
xue2024wdmoe,1,Wdmoe: Wireless distributed large language models with mixture of experts,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(8)
xue2024openmoe,1,Openmoe: An early effort on open mixture-of-experts language models,A Survey on MoE in LLM(36)
xue2022one,1,One student knows all experts know: From sparse to dense,A Survey on MoE in LLM(50)
xue2022go,1,Go wider instead of deeper,A Survey on MoE in LLM(56)
xu2024device,1,On-device language models: A comprehensive review,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(3)
xu2023llmcad,1,Llmcad: Fast and scalable on-device large language model inference,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(11)
wu2024yuan,1,Yuan 2.0-m32: Mixture of experts with attention router,A Survey on MoE in LLM(70)
wu2024omni,1,Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts,A Survey on MoE in LLM(41)
wu2024mixture,1,Mixture of lora experts,A Survey on MoE in LLM(46)
wu2022residual,1,Residual mixture of experts,A Survey on MoE in LLM(99)
wei2024skywork,1,Skywork-moe: A deep dive into training techniques for mixture-of-experts language models,A Survey on MoE in LLM(65)
wei2022emergent,1,Emergent abilities of large language models,A Survey on MoE in LLM(12)
wang2023fusing,1,Fusing models with complementary expertise,A Survey on MoE in LLM(103)
wang2022adamix,1,Adamix: Mixture-of-adaptations for parameter-efficient model tuning,A Survey on MoE in LLM(42)
vaswani2017attention,1,Attention is all you need,A Survey on MoE in LLM(1)
theis2015generative,1,Generative image modeling using spatial lstms,A Survey on MoE in LLM(21)
team2024gemma,1,Gemma 2: Improving open language models at a practical size,A Survey on MoE in LLM(27)
tang2020progressive,1,Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations,A Survey on MoE in LLM(124)
tan2024scattered,1,Scattered mixture-of-experts implementation,A Survey on MoE in LLM(113)
tan2023sparse,1,Sparse universal transformer,A Survey on MoE in LLM(57)
sukhbaatar2024branch,1,Branch-train-mix: Mixing expert llms into a mixture-of-experts llm,A Survey on MoE in LLM(52)
snowflake2024research,1,"Research. Snowflake Arctic: The best LLM for enterprise AIâ€”efficiently intelligent, truly open",A Survey on MoE in LLM(29)
singh2023hybrid,1,A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training,A Survey on MoE in LLM(108)
shen2025will,1,Will llms scaling hit the wall? breaking barriers via distributed resources on massive edge devices,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(5)
shen2024jetmoe,1,Jetmoe: Reaching llama2 performance with 0.1 m dollars,A Survey on MoE in LLM(89)
shen2023moduleformer,1,ModuleFormer: Modularity Emerges from Mixture-of-Experts,A Survey on MoE in LLM(71)
shazeer2017outrageously,1,Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,A Survey on MoE in LLM(24)
shahbaba2009nonlinear,1,Nonlinear models using Dirichlet process mixtures.,A Survey on MoE in LLM(19)
roller2021hash,1,Hash layers for large sparse models,A Survey on MoE in LLM(80)
riquelme2021scaling,1,Scaling vision with sparse mixture of experts,A Survey on MoE in LLM(6)
ren2023pangu,1,Pangu-$\{$$\backslash$Sigma$\}$: Towards trillion parameter language model with sparse heterogeneous computing,A Survey on MoE in LLM(84)
rasmussen2001infinite,1,Infinite mixtures of Gaussian process experts,A Survey on MoE in LLM(18)
raposo2024mixture,1,Mixture-of-depths: Dynamically allocating compute in transformer-based language models,A Survey on MoE in LLM(55)
rajbhandari2022deepspeed,1,Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale,A Survey on MoE in LLM(64)
qu2025mobile,1,Mobile edge intelligence for large language models: A contemporary survey,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(12)
puigcerver2023sparse,1,From sparse to soft mixtures of experts,A Survey on MoE in LLM(37)
pan2024dense,1,"Dense training, sparse inference: Rethinking training of mixture-of-experts language models",A Survey on MoE in LLM(62)
nie2023flexmoe,1,Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement,A Survey on MoE in LLM(110)
nie2022hetumoe,1,HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system,A Survey on MoE in LLM(109)
nie2021evomoe,1,Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate,A Survey on MoE in LLM(60)
muqeeth2023soft,1,Soft merging of experts with adaptive routing,A Survey on MoE in LLM(38)
minaee2024large,1,Large language models: A survey,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(4)
mao2025hope,1,HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts,A Survey on MoE in LLM(87)
mao2022unipelt,1,Unipelt: A unified framework for parameter-efficient language model tuning,A Survey on MoE in LLM(96)
ma2018modeling,1,Modeling task relationships in multi-task learning with multi-gate mixture-of-experts,A Survey on MoE in LLM(59)
luo2024moelora,1,Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models,A Survey on MoE in LLM(45)
lu2019vilbert,1,Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,A Survey on MoE in LLM(8)
liu2024deepseek,1,"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",A Survey on MoE in LLM(30)
liu2021swin,1,Swin transformer: Hierarchical vision transformer using shifted windows,A Survey on MoE in LLM(7)
lin2024moe,1,Moe-llava: Mixture of experts for large vision-language models,A Survey on MoE in LLM(91)
lieber2024jamba,1,Jamba: A hybrid transformer-mamba language model,A Survey on MoE in LLM(66)
li2022branch,1,Branch-train-merge: Embarrassingly parallel training of expert language models,A Survey on MoE in LLM(102)
lewis2021base,1,"Base layers: Simplifying training of large, sparse models",A Survey on MoE in LLM(72)
lepikhin2020gshard,1,Gshard: Scaling giant models with conditional computation and automatic sharding,A Survey on MoE in LLM(25)
kudugunta2021beyond,1,Beyond distillation: Task-level mixture-of-experts for efficient inference,A Survey on MoE in LLM(75)
komatsuzaki2022sparse,1,Sparse upcycling: Training mixture-of-experts from dense checkpoints,A Survey on MoE in LLM(47)
kim2021scalable,1,Scalable and efficient moe training for multitask multilingual models,A Survey on MoE in LLM(74)
kaplan2020scaling,1,Scaling laws for neural language models,A Survey on MoE in LLM(11)
jordan1994hierarchical,1,Hierarchical mixtures of experts and the EM algorithm,A Survey on MoE in LLM(16)
jin2025moe,1,MoE $\^{} 2$: Optimizing Collaborative Inference for Edge Large Language Models,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(15)
jiang2024survey,1,A survey on large language models for code generation,A Survey on MoE in LLM(5)
jiang2024mixtral,1,Mixtral of experts,A Survey on MoE in LLM(26)
jiang2024lancet,1,Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping,A Survey on MoE in LLM(118)
jiang2023adamct,1,AdaMCT: adaptive mixture of CNN-transformer for sequential recommendation,A Survey on MoE in LLM(125)
jacobs1991adaptive,1,Adaptive mixtures of local experts,A Survey on MoE in LLM(15)
hwang2024pre,1,Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference,A Survey on MoE in LLM(120)
hwang2023tutel,1,Tutel: Adaptive mixture-of-experts at scale,A Survey on MoE in LLM(105)
huang2023experts,1,Experts weights averaging: A new general training scheme for vision transformers,A Survey on MoE in LLM(101)
hu2024rdma,1,RDMA transports in datacenter networks: survey,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(10)
hosseinzadeh2025dilemma,1,DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(14)
hoffmann2022training,1,Training compute-optimal large language models,A Survey on MoE in LLM(14)
he2022fastermoe,1,Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models,A Survey on MoE in LLM(107)
he2021fastmoe,1,Fastmoe: A fast mixture-of-expert training system,A Survey on MoE in LLM(104)
hazimeh2021dselect,1,Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning,A Survey on MoE in LLM(73)
gururangan2022demix,1,Demix layers: Disentangling domains for modular language modeling,A Survey on MoE in LLM(82)
gou2023mixture,1,Mixture of cluster-conditional lora experts for vision-language instruction tuning,A Survey on MoE in LLM(44)
gao2024higher,1,Higher layers need more lora experts,A Survey on MoE in LLM(97)
gale2023megablocks,1,Megablocks: Efficient sparse training with mixture-of-experts,A Survey on MoE in LLM(112)
fedus2022switch,1,Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,A Survey on MoE in LLM(34)
fedus2022review,1,A review of sparse expert models in deep learning,A Survey on MoE in LLM(32)
fan2021beyond,1,Beyond english-centric multilingual machine translation,A Survey on MoE in LLM(83)
eigen2013learning,1,Learning factored representations in a deep mixture of experts,A Survey on MoE in LLM(20)
dua2022tricks,1,Tricks for training sparse translation models,A Survey on MoE in LLM(100)
du2022glam,1,Glam: Efficient scaling of language models with mixture-of-experts,A Survey on MoE in LLM(33)
dou2023loramoe,1,Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment,A Survey on MoE in LLM(43)
diao2023mixture,1,Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories,A Survey on MoE in LLM(92)
deisenroth2015distributed,1,Distributed gaussian processes,A Survey on MoE in LLM(22)
dai2024high,1,High-speed data communication with advanced networks in large language model training,The MoE-empowered edge LLMS deployment Architecture challenges and opportunities(9)
dai2024deepseekmoe,1,Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,A Survey on MoE in LLM(67)
dai2022stablemoe,1,Stablemoe: Stable routing strategy for mixture of experts,A Survey on MoE in LLM(79)
costa2022no,1,No language left behind: Scaling human-centered machine translation,A Survey on MoE in LLM(76)
collobert2001parallel,1,A parallel mixture of SVMs for very large scale problems,A Survey on MoE in LLM(17)
clark2022unified,1,Unified scaling laws for routed language models,A Survey on MoE in LLM(63)
chowdhury2023patch,1,Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks,A Survey on MoE in LLM(123)
chowdhery2023palm,1,Palm: Scaling language modeling with pathways,A Survey on MoE in LLM(3)
choi2023smop,1,Smop: Towards efficient and effective prompt tuning with sparse mixture-of-prompts,A Survey on MoE in LLM(58)
chi2022representation,1,On the representation collapse of sparse mixture of experts,A Survey on MoE in LLM(77)
chen2024llava,1,Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms,A Survey on MoE in LLM(94)
chen2023mod,1,Mod-squad: Designing mixtures of experts as modular multi-task learners,A Survey on MoE in LLM(69)
chen2023lifelong,1,Lifelong language pretraining with distribution-specialized experts,A Survey on MoE in LLM(53)
chen2022task,1,Task-specific expert pruning for sparse mixture-of-experts,A Survey on MoE in LLM(51)
chen2022ta,1,Ta-moe: Topology-aware large scale mixture-of-expert training,A Survey on MoE in LLM(116)
cai2024shortcut,1,Shortcut-connected expert parallelism for accelerating mixture-of-experts,A Survey on MoE in LLM(119)
brown2020language,1,Language models are few-shot learners,A Survey on MoE in LLM(2)
artetxe2021efficient,1,Efficient large scale language modeling with mixtures of experts,A Survey on MoE in LLM(90)
antoniak2024mixture,1,Mixture of tokens: Continuous moe through cross-example aggregation,A Survey on MoE in LLM(54)
aljundi2017expert,1,Expert gate: Lifelong learning with a network of experts,A Survey on MoE in LLM(23)
achiam2023gpt,1,Gpt-4 technical report,A Survey on MoE in LLM(4)
